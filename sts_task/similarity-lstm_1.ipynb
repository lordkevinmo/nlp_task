{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "\n",
    "import torch\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split as split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    def __init__(self, data_name, data_file, train_ratio=0.8, max_len=None,\n",
    "               vocab_limit=None, sentence_cols=None, score_col=None):\n",
    "        self.data_file = data_file\n",
    "        self.train_ratio = train_ratio\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = 1\n",
    "        self.vocab_limit = vocab_limit\n",
    "        \n",
    "        if data_name.lower() == 'sick': # if sick dataset\n",
    "            self.score_col = 'relatedness_score'\n",
    "            self.sequence_cols = ['sentence_A', 'sentence_B']\n",
    "        elif data_name.lower() == 'quora': #if quora dataset\n",
    "            self.score_col = 'is_duplicate'\n",
    "            self.sequence_cols = ['question1', 'question2']\n",
    "        else:\n",
    "            self.score_col = score_col\n",
    "            self.sequence_cols = question_cols\n",
    "        \n",
    "        self.x_train = list()\n",
    "        self.y_train = list()\n",
    "        self.x_val = list()\n",
    "        self.y_val = list()\n",
    "        self.vocab = set('PAD')\n",
    "        self.word2index = {'PAD':0}\n",
    "        self.index2word = {0:'PAD'}\n",
    "        self.word2count = dict()\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.run()\n",
    "        \n",
    "    def text_to_word_list(self, text):\n",
    "        ''' Pre process and convert texts to a list of words '''\n",
    "        text = str(text)\n",
    "        text = text.lower()\n",
    "\n",
    "        # Clean the text\n",
    "        text = sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = sub(r\"what's\", \"what is \", text)\n",
    "        text = sub(r\"\\'s\", \" \", text)\n",
    "        text = sub(r\"\\'ve\", \" have \", text)\n",
    "        text = sub(r\"can't\", \"cannot \", text)\n",
    "        text = sub(r\"n't\", \" not \", text)\n",
    "        text = sub(r\"i'm\", \"i am \", text)\n",
    "        text = sub(r\"\\'re\", \" are \", text)\n",
    "        text = sub(r\"\\'d\", \" would \", text)\n",
    "        text = sub(r\"\\'ll\", \" will \", text)\n",
    "        text = sub(r\",\", \" \", text)\n",
    "        text = sub(r\"\\.\", \" \", text)\n",
    "        text = sub(r\"!\", \" ! \", text)\n",
    "        text = sub(r\"\\/\", \" \", text)\n",
    "        text = sub(r\"\\^\", \" ^ \", text)\n",
    "        text = sub(r\"\\+\", \" + \", text)\n",
    "        text = sub(r\"\\-\", \" - \", text)\n",
    "        text = sub(r\"\\=\", \" = \", text)\n",
    "        text = sub(r\"'\", \" \", text)\n",
    "        text = sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = sub(r\":\", \" : \", text)\n",
    "        text = sub(r\" e g \", \" eg \", text)\n",
    "        text = sub(r\" b g \", \" bg \", text)\n",
    "        text = sub(r\" u s \", \" american \", text)\n",
    "        text = sub(r\"\\0s\", \"0\", text)\n",
    "        text = sub(r\" 9 11 \", \"911\", text)\n",
    "        text = sub(r\"e - mail\", \"email\", text)\n",
    "        text = sub(r\"j k\", \"jk\", text)\n",
    "        text = sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "        text = text.split()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def load_data(self):\n",
    "        stops = set(stopwords.words('english'))\n",
    "\n",
    "        # Load data set\n",
    "        data_df = pd.read_csv(self.data_file, sep='\\t')\n",
    "\n",
    "        # Iterate over required sequences of provided dataset\n",
    "        for index, row in data_df.iterrows():\n",
    "            # Iterate through the text of both questions of the row\n",
    "            for sequence in self.sequence_cols:\n",
    "                s2n = []  # Sequences with words replaces with indices\n",
    "                for word in self.text_to_word_list(row[sequence]):\n",
    "                    # Remove unwanted words\n",
    "                    if word in stops:\n",
    "                        continue\n",
    "\n",
    "                    if word not in self.vocab:\n",
    "                        self.vocab.add(word)\n",
    "                        self.word2index[word] = self.vocab_size\n",
    "                        self.word2count[word] = 1\n",
    "                        s2n.append(self.vocab_size)\n",
    "                        self.index2word[self.vocab_size] = word\n",
    "                        self.vocab_size += 1\n",
    "                    else:\n",
    "                        self.word2count[word] += 1\n",
    "                        s2n.append(self.word2index[word])\n",
    "\n",
    "                # Replace |sequence as word| with |sequence as number| representation\n",
    "                data_df.at[index, sequence] = s2n\n",
    "        return data_df\n",
    "    \n",
    "    def convert_to_tensors(self):\n",
    "        for data in [self.x_train, self.x_val]:\n",
    "            for i, pair in enumerate(data):\n",
    "                data[i][0] = torch.LongTensor(data[i][0])\n",
    "                data[i][1] = torch.LongTensor(data[i][1])\n",
    "\n",
    "                if self.use_cuda:\n",
    "                    data[i][0] = data[i][0].cuda()\n",
    "                    data[i][1] = data[i][1].cuda()\n",
    "\n",
    "        self.y_train = torch.FloatTensor(self.y_train)\n",
    "        self.y_val = torch.FloatTensor(self.y_val)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            self.y_train = self.y_train.cuda()\n",
    "            self.y_val = self.y_val.cuda()\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        # Loading data and building vocabulary.\n",
    "        data_df = self.load_data()\n",
    "        data_size = len(data_df)\n",
    "\n",
    "        X = data_df[self.sequence_cols]\n",
    "        Y = data_df[self.score_col]\n",
    "\n",
    "        self.x_train, self.x_val, self.y_train, self.y_val = split_data(X, Y, train_size=self.train_ratio)\n",
    "\n",
    "        # Convert labels to their numpy representations\n",
    "        self.y_train = self.y_train.values\n",
    "        self.y_val = self.y_val.values\n",
    "\n",
    "        training_pairs = []\n",
    "        training_scores = []\n",
    "        validation_pairs = []\n",
    "        validation_scores = []\n",
    "\n",
    "        # Split to lists\n",
    "        i = 0\n",
    "        for index, row in self.x_train.iterrows():\n",
    "            sequence_1 = row[self.sequence_cols[0]]\n",
    "            sequence_2 = row[self.sequence_cols[1]]\n",
    "            if len(sequence_1) > 0 and len(sequence_2) > 0:\n",
    "                training_pairs.append([sequence_1, sequence_2])\n",
    "                training_scores.append(float(self.y_train[i]))\n",
    "            i += 1\n",
    "        self.x_train = training_pairs\n",
    "        self.y_train = training_scores\n",
    "\n",
    "        print('Number of Training Positive Samples   :', sum(training_scores))\n",
    "        print('Number of Training Negative Samples   :', len(training_scores) - sum(training_scores))\n",
    "\n",
    "        i = 0\n",
    "        for index, row in self.x_val.iterrows():\n",
    "            sequence_1 = row[self.sequence_cols[0]]\n",
    "            sequence_2 = row[self.sequence_cols[1]]\n",
    "            if len(sequence_1) > 0 and len(sequence_2) > 0:\n",
    "                validation_pairs.append([sequence_1, sequence_2])\n",
    "                validation_scores.append(float(self.y_val[i]))\n",
    "            i += 1\n",
    "\n",
    "        self.x_val = validation_pairs\n",
    "        self.y_val = validation_scores\n",
    "\n",
    "        print('Number of Validation Positive Samples   :', sum(validation_scores))\n",
    "        print('Number of Validation Negative Samples   :', len(validation_scores) - sum(validation_scores))\n",
    "\n",
    "        assert len(self.x_train) == len(self.y_train)\n",
    "        assert len(self.x_val) == len(self.y_val)\n",
    "\n",
    "        self.convert_to_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Get_Embedding(object):\n",
    "    def __init__(self, file_path, word_index):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.embedding_size = 300 # Dimensionality of Google News' Word2Vec\n",
    "        self.embedding_matrix = self.create_embed_matrix(file_path, word_index)\n",
    "\n",
    "    def create_embed_matrix(self, file_path, word_index):\n",
    "        word2vec = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "\n",
    "        # Prepare Embedding Matrix.\n",
    "        embedding_matrix = np.zeros((len(word_index)+1, self.embedding_size))\n",
    "\n",
    "        for word, i in word_index.items():\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            if word not in word2vec.vocab:\n",
    "                continue\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "\n",
    "        del word2vec\n",
    "\n",
    "        embedding_matrix = torch.FloatTensor(embedding_matrix)\n",
    "        if self.use_cuda: embedding_matrix = embedding_matrix.cuda()\n",
    "\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the LSTM model\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Manhattan_LSTM(nn.Module):\n",
    "    def __init__(self, data_name, hidden_size, embedding, use_embedding=False, train_embedding=True):\n",
    "        super(Manhattan_LSTM, self).__init__()\n",
    "        self.data_name = data_name\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if use_embedding:\n",
    "            self.embedding = nn.Embedding(embedding.shape[0], embedding.shape[1])\n",
    "            self.embedding.weight = nn.Parameter(embedding)\n",
    "            self.input_size = embedding.shape[1] # V - Size of embedding vector\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(embedding[0], embedding[1])\n",
    "            self.input_size = embedding[1]\n",
    "\n",
    "        self.embedding.weight.requires_grad = train_embedding\n",
    "\n",
    "        self.lstm_1 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.lstm_2 = nn.LSTM(self.input_size, self.hidden_size, num_layers=1, bidirectional=True)\n",
    "\n",
    "    def exponent_neg_manhattan_distance(self, x1, x2):\n",
    "        ''' Helper function for the similarity estimate of the LSTMs outputs '''\n",
    "        return torch.exp(-torch.sum(torch.abs(x1 - x2), dim=1))\n",
    "\n",
    "    def forward(self, input_val, hidden):\n",
    "        '''\n",
    "        input           -> (2 x Max. Sequence Length (per batch) x Batch Size)\n",
    "        hidden          -> (2 x Num. Layers * Num. Directions x Batch Size x Hidden Size)\n",
    "        '''\n",
    "        embedded_1 = self.embedding(input_val[0]) # L, B, V\n",
    "        embedded_2 = self.embedding(input_val[1]) # L, B, V\n",
    "\n",
    "        batch_size = embedded_1.size()[1]\n",
    "\n",
    "        outputs_1, hidden_1 = self.lstm_1(embedded_1, hidden)\n",
    "        outputs_2, hidden_2 = self.lstm_2(embedded_2, hidden)\n",
    "\n",
    "        similarity_scores = self.exponent_neg_manhattan_distance(hidden_1[0].permute(1, 2, 0) .contiguous().view(batch_size, -1),\n",
    "                                                                 hidden_2[0].permute(1, 2, 0) .contiguous().view(batch_size, -1))\n",
    "\n",
    "        if self.data_name == 'sick': return similarity_scores*5.0\n",
    "        else: return similarity_scores\n",
    "\n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights of lstm 1 '''\n",
    "        for name_1, param_1 in self.lstm_1.named_parameters():\n",
    "            if 'bias' in name_1:\n",
    "                nn.init.constant_(param_1, 0.0)\n",
    "            elif 'weight' in name_1:\n",
    "                nn.init.xavier_normal_(param_1)\n",
    "\n",
    "        ''' Set weights of lstm 2 identical to lstm 1 '''\n",
    "        lstm_1 = self.lstm_1.state_dict()\n",
    "        lstm_2 = self.lstm_2.state_dict()\n",
    "\n",
    "        for name_1, param_1 in lstm_1.items():\n",
    "            # Backwards compatibility for serialized parameters.\n",
    "            if isinstance(param_1, torch.nn.Parameter):\n",
    "                param_1 = param_1.data\n",
    "\n",
    "            lstm_2[name_1].copy_(param_1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Hidden dimensionality : 2 (h_0, c_0) x Num. Layers * Num. Directions x Batch Size x Hidden Size\n",
    "        # result = torch.zeros(2, 1, batch_size, self.hidden_size)\n",
    "        if self.use_cuda:\n",
    "            result = (torch.zeros(2, batch_size, self.hidden_size).cuda(),\n",
    "                      torch.zeros(2, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            result = (torch.zeros(2, batch_size, self.hidden_size), torch.zeros(1, batch_size, self.hidden_size))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "class Helper(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def as_minutes(self, s):\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    def time_slice(self, since, percent):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        es = s / (percent)\n",
    "        rs = es - s\n",
    "        return '%s (- %s)' % (self.as_minutes(s), self.as_minutes(rs))\n",
    "\n",
    "    def show_plot(self, points):\n",
    "        plt.figure()\n",
    "        fig, ax = plt.subplots()\n",
    "        # this locator puts ticks at regular intervals\n",
    "        loc = ticker.MultipleLocator(base=0.2)\n",
    "        ax.yaxis.set_major_locator(loc)\n",
    "        plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "from nltk import bleu_score\n",
    "\n",
    "class Run_Iterations(object):\n",
    "    def __init__(self, data_name, model, x_train, y_train, index2word, batch_size, num_iters,\n",
    "                 learning_rate, tracking_pair=False, x_val=[], y_val=[], print_every=1, plot_every=1):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        self.data_name = data_name\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.num_iters = num_iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        self.tracking_pair = tracking_pair\n",
    "        self.print_every = print_every\n",
    "        self.plot_every = plot_every\n",
    "\n",
    "        self.index2word = index2word\n",
    "        ''' Lists that will contain data in the form of tensors. '''\n",
    "        # Training data.\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.train_samples = len(self.x_train)\n",
    "\n",
    "        # Development data.\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.val_samples = len(self.x_val)\n",
    "\n",
    "        self.help_fn = Helper()\n",
    "\n",
    "    def train_iters(self):\n",
    "        start = time.time()\n",
    "        plot_losses = []\n",
    "        print_loss_total = 0.0  # Reset every self.print_every\n",
    "        plot_loss_total = 0.0  # Reset every self.plot_every\n",
    "\n",
    "        model_trainable_parameters = list(filter(lambda p: p.requires_grad, self.model.manhattan_lstm.parameters()))\n",
    "        model_optimizer = optim.Adam(model_trainable_parameters, lr=self.learning_rate)\n",
    "\n",
    "        print('Beginning Model Training.\\n')\n",
    "\n",
    "        for epoch in range(1, self.num_iters + 1):\n",
    "            for i in range(0, self.train_samples, self.batch_size):\n",
    "                input_variables = self.x_train[i : i + self.batch_size] # Batch Size x Sequence Length\n",
    "                similarity_scores = self.y_train[i : i + self.batch_size] # Batch Size\n",
    "\n",
    "                loss, _ = self.model.train(input_variables, similarity_scores, self.criterion, model_optimizer)\n",
    "                print_loss_total += loss\n",
    "                plot_loss_total += loss\n",
    "\n",
    "            if epoch % self.print_every == 0:\n",
    "                print_loss_avg = print_loss_total / self.print_every\n",
    "                print_loss_total = 0\n",
    "                print('%s (%d %d%%) %.4f' % (self.help_fn.time_slice(start, epoch / self.num_iters),\n",
    "                                             epoch, epoch / self.num_iters * 100, print_loss_avg))\n",
    "\n",
    "            if epoch % self.plot_every == 0:\n",
    "                plot_loss_avg = plot_loss_total / self.plot_every\n",
    "                plot_losses.append(plot_loss_avg)\n",
    "                plot_loss_total = 0\n",
    "\n",
    "            print('Validation Accuracy: %f Validation Precision: %f Validation Recall: %f Validation Loss: %f' % self.get_accuracy())\n",
    "            print('\\n')\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                self.learning_rate *= 0.80\n",
    "                model_optimizer = optim.Adam(model_trainable_parameters, lr=self.learning_rate)\n",
    "\n",
    "        self.help_fn.show_plot(plot_losses)\n",
    "\n",
    "    def evaluate(self, seqs, scores):\n",
    "        loss, similarity_scores = self.model.train(seqs, scores, self.criterion, evaluate=True)\n",
    "        return loss, similarity_scores\n",
    "\n",
    "    def evaluate_specific(self, seqs, score, name='tracking_pair'):\n",
    "        sequence1 = [self.index2word[j.item()] for j in seqs[0].view(-1).data]\n",
    "        sequence2 = [self.index2word[j.item()] for j in seqs[1].view(-1).data]\n",
    "        print('>', sequence1)\n",
    "        print('>', sequence2)\n",
    "        print('=', score.item())\n",
    "\n",
    "        _, similarity_score = self.evaluate([seqs], score)\n",
    "        print('<', similarity_score.item())\n",
    "\n",
    "    def evaluate_randomly(self, n=10):\n",
    "        for i in range(n):\n",
    "            ind = random.randrange(self.val_samples)\n",
    "            self.evaluate_specific(self.x_val[ind], self.y_val[ind], name=str(i))\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        true_positive = 0\n",
    "        true_negative = 0\n",
    "        false_positive = 0\n",
    "        false_negative = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        accuracy = 0.0\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "\n",
    "        scale = 1.0\n",
    "        if self.data_name == 'sick': scale *= 5.0 #if sick dataset\n",
    "\n",
    "        for i in range(0, self.val_samples, self.batch_size):\n",
    "            input_variables = self.x_val[i : i + self.batch_size] # Batch Size x Sequence Length\n",
    "            actual_scores = self.y_val[i : i + self.batch_size] # Batch Size\n",
    "\n",
    "            loss, predicted_scores = self.model.train(input_variables, actual_scores, self.criterion, evaluate=True)\n",
    "            total_loss += loss\n",
    "\n",
    "            for actual, predict in zip(actual_scores, predicted_scores):\n",
    "                if actual.item()/scale < 0.5 and predict.item() < 0.5:\n",
    "                    true_negative += 1\n",
    "\n",
    "                if actual.item()/scale < 0.5 and predict.item() >= 0.5:\n",
    "                    false_positive += 1\n",
    "\n",
    "                elif actual.item()/scale >= 0.5 and predict.item() >= 0.5:\n",
    "                    true_positive += 1\n",
    "\n",
    "                if actual.item()/scale >= 0.5 and predict.item() < 0.5:\n",
    "                    false_negative += 1\n",
    "\n",
    "        accuracy = (true_positive + true_negative)*100/len(self.x_val)\n",
    "        if true_positive + false_positive > 0: precision = true_positive*100/(true_positive + false_positive)\n",
    "        if true_positive + false_negative > 0: recall = true_positive*100/(true_positive + false_negative)\n",
    "\n",
    "        return accuracy, precision, recall, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class Train_Network(object):\n",
    "    def __init__(self, manhattan_lstm, index2word):\n",
    "        self.manhattan_lstm = manhattan_lstm\n",
    "        self.index2word = index2word\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    def train(self, input_sequences, similarity_scores, criterion, model_optimizer=None, evaluate=False):\n",
    "\n",
    "        sequences_1 = [sequence[0] for sequence in input_sequences]\n",
    "        sequences_2 = [sequence[1] for sequence in input_sequences]\n",
    "        batch_size = len(sequences_1)\n",
    "\n",
    "        '''\n",
    "        Pad all tensors in this batch to same length.\n",
    "        PyTorch pad_sequence method doesn't take pad length, making this step problematic.\n",
    "        Therefore, lists concatenated, padded to common length, and then split.\n",
    "        '''\n",
    "        temp = rnn.pad_sequence(sequences_1 + sequences_2)\n",
    "        sequences_1 = temp[:, :batch_size]\n",
    "        sequences_2 = temp[:, batch_size:]\n",
    "\n",
    "        ''' No need to send optimizer in case of evaluation. '''\n",
    "        if model_optimizer: model_optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "\n",
    "        hidden = self.manhattan_lstm.init_hidden(batch_size)\n",
    "        output_scores = self.manhattan_lstm([sequences_1, sequences_2], hidden).view(-1)\n",
    "\n",
    "        loss += criterion(output_scores, similarity_scores)\n",
    "\n",
    "        if not evaluate:\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "\n",
    "        return loss.item(), output_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters:\n",
      "Hidden Size                  : 50\n",
      "Batch Size                   : 32\n",
      "Max. input length            : 20\n",
      "Learning rate                : 0.003\n",
      "Number of Epochs             : 7\n",
      "--------------------------------------\n",
      "\n",
      "Reading Data.\n",
      "Number of Training Positive Samples   : 119345.0\n",
      "Number of Training Negative Samples   : 204022.0\n",
      "Number of Validation Positive Samples   : 29957.0\n",
      "Number of Validation Negative Samples   : 50894.0\n",
      "\n",
      "\n",
      "Number of training samples        : 323367\n",
      "Number of validation samples      : 80851\n",
      "Maximum sequence length           : 20\n",
      "\n",
      "\n",
      "Building Embedding Matrix\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "data_name = \"quora\"\n",
    "data_file = \"./quora.tsv\"\n",
    "embd_file = \"./GoogleNews-vectors-negative300.bin.gz\"\n",
    "training_ratio = 0.8\n",
    "max_len = 20\n",
    "tracking_pair = False\n",
    "hidden_size = 50\n",
    "batch_size = 32\n",
    "num_iters = 7\n",
    "learning_rate = 0.003\n",
    "\n",
    "print('Model Parameters:')\n",
    "print('Hidden Size                  :', hidden_size)\n",
    "print('Batch Size                   :', batch_size)\n",
    "print('Max. input length            :', max_len)\n",
    "print('Learning rate                :', learning_rate)\n",
    "print('Number of Epochs             :', num_iters)\n",
    "print('--------------------------------------\\n')\n",
    "\n",
    "print('Reading Data.')\n",
    "data = Data(data_name, data_file, training_ratio, max_len)\n",
    "\n",
    "print('\\n')\n",
    "print('Number of training samples        :', len(data.x_train))\n",
    "print('Number of validation samples      :', len(data.x_val))\n",
    "print('Maximum sequence length           :', max_len)\n",
    "print('\\n')\n",
    "\n",
    "print('Building Embedding Matrix')\n",
    "embedding = Get_Embedding(embd_file, data.word2index)\n",
    "embedding_size = embedding.embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model.\n",
      "Training Network.\n",
      "Beginning Model Training.\n",
      "\n",
      "5m 13s (- 31m 21s) (1 14%) 3729.8734\n",
      "Validation Accuracy: 62.947892 Validation Precision: 0.000000 Validation Recall: 0.000000 Validation Loss: 936.348641\n",
      "\n",
      "\n",
      "11m 27s (- 28m 37s) (2 28%) 2624.2041\n",
      "Validation Accuracy: 75.315086 Validation Precision: 70.852972 Validation Recall: 56.704610 Validation Loss: 426.905084\n",
      "\n",
      "\n",
      "17m 58s (- 23m 57s) (3 42%) 1551.5964\n",
      "Validation Accuracy: 76.083165 Validation Precision: 70.237823 Validation Recall: 61.518176 Validation Loss: 415.143936\n",
      "\n",
      "\n",
      "24m 26s (- 18m 20s) (4 57%) 1359.9104\n",
      "Validation Accuracy: 75.846928 Validation Precision: 68.331869 Validation Recall: 64.882999 Validation Loss: 421.806081\n",
      "\n",
      "\n",
      "30m 50s (- 12m 20s) (5 71%) 1240.0588\n",
      "Validation Accuracy: 75.673770 Validation Precision: 68.043913 Validation Recall: 64.759489 Validation Loss: 428.341028\n",
      "\n",
      "\n",
      "37m 1s (- 6m 10s) (6 85%) 1121.8868\n",
      "Validation Accuracy: 76.177165 Validation Precision: 69.309648 Validation Recall: 64.078513 Validation Loss: 423.922377\n",
      "\n",
      "\n",
      "43m 14s (- 0m 0s) (7 100%) 1038.5873\n",
      "Validation Accuracy: 76.154902 Validation Precision: 68.511199 Validation Recall: 65.961211 Validation Loss: 427.134037\n",
      "\n",
      "\n",
      "> ['fairly', 'painless', 'ways', 'die']\n",
      "> ['easiest', 'painless', 'way', 'die']\n",
      "= 1.0\n",
      "< 0.00031095868325792253\n",
      "> ['differences', 'mainland', 'china', 'chinese', 'taipei']\n",
      "> ['cultural', 'difference', 'mainland', 'china', 'thailand']\n",
      "= 0.0\n",
      "< 2.590986778426563e-13\n",
      "> ['best', 'phone', 'buy', '15000']\n",
      "> ['best', 'smartphone', 'buy', '15000', 'inr']\n",
      "= 1.0\n",
      "< 0.005039616487920284\n",
      "> ['difference', 'aesthetic', 'bodybuilding', 'normal', 'bodybuilding']\n",
      "> ['purpose', 'bodybuilding']\n",
      "= 0.0\n",
      "< 7.666743386252218e-15\n",
      "> ['came', 'know', '13', 'years', 'adopted']\n",
      "> ['came', 'know', '14', 'years', 'adopted']\n",
      "= 1.0\n",
      "< 1.2171574326202972e-07\n",
      "> ['important', 'sound', 'design', 'films']\n",
      "> ['involved', 'sound', 'design', 'film']\n",
      "= 1.0\n",
      "< 1.9536032947603532e-12\n",
      "> ['one', 'write', 'python', 'library']\n",
      "> ['want', 'write', 'python', 'library', 'work', 'proper', 'steps']\n",
      "= 1.0\n",
      "< 2.3574550644944736e-10\n",
      "> ['become', 'good', 'coder', '2', 'months']\n",
      "> ['become', 'good', 'coder', '7', 'months']\n",
      "= 0.0\n",
      "< 3.7685470033466117e-06\n",
      "> ['good', 'antivirus', 'thor']\n",
      "> ['sophos', 'antivirus', 'good', 'mac']\n",
      "= 0.0\n",
      "< 1.577011633259462e-12\n",
      "> ['ipad', 'charging', 'fixed']\n",
      "> ['ipad', 'charging']\n",
      "= 1.0\n",
      "< 0.024570312350988388\n"
     ]
    }
   ],
   "source": [
    "print('Building model.')\n",
    "model = Manhattan_LSTM(data_name, hidden_size, embedding.embedding_matrix, use_embedding=True, train_embedding=True)\n",
    "if use_cuda: model = model.cuda()\n",
    "\n",
    "model.init_weights()\n",
    "\n",
    "print(\"Training Network.\")\n",
    "train_network = Train_Network(model, data.index2word)\n",
    "\n",
    "run_iterations = Run_Iterations(data_name, train_network, data.x_train, data.y_train, data.index2word,\n",
    "                                batch_size, num_iters, learning_rate,\n",
    "                                tracking_pair=tracking_pair, x_val=data.x_val, y_val=data.y_val)\n",
    "run_iterations.train_iters()\n",
    "run_iterations.evaluate_randomly()\n",
    "\n",
    "torch.save(model.state_dict(), './manhattan_lstm.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
