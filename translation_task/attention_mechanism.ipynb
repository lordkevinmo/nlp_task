{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, drop_prob=0):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=drop_prob, batch_first=True)\n",
    "    \n",
    "    def forward(self, inputs, hidden):\n",
    "        # Embed input words\n",
    "        embedded = self.embedding(inputs)\n",
    "        # Pass the embedded word vectors into LSTM and return all outputs\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculating Alignment Scores\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahDanauDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, drop_prob=0.1):\n",
    "        super(BahDanauDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.lstm = nn.LSTM(self.hidden_size * 2, self.hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        encoder_outputs = encoder_outputs.squeeze()\n",
    "        # Embed input words\n",
    "        embedded = self.embedding(inputs).view(1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Calculating Alignment Scores\n",
    "        x = torch.tanh(self.fc_hidden(hidden[0])+self.fc_encoder(encoder_outputs))\n",
    "        alignment_scores = x.bmm(self.weight.unsquezze(2))\n",
    "        \n",
    "        # Softmaxing alignment scores to get Attention weights\n",
    "        attn_weights = F.softmax(alignment_scores.view(1, -1), dim=1)\n",
    "        \n",
    "        # Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                  encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # Concatening context vector with embedded input word\n",
    "        output = torch.cat((embedded, context_vector[0]), 1).unsqueeze(0)\n",
    "        # Passing the concatened vector as input to the LSTM cell\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # Passing the lstm output through a Linear layer acting as a classifier\n",
    "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, attention, n_layers=1, drop_prob=0.1):\n",
    "        super(LuongDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # The Attention Mechanism is defined in a separate class\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.drop_prob)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.classifier = nn.Linear(self.hidden_size*2, self.output_size)\n",
    "    \n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        # Embed input words\n",
    "        embedded = self.embedding(inputs).view(1,1,-1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Passing previous output word (embedded) and hidden state into LSTM cell\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        # Calculating Alignment Scores - see Attention class for the forward pass function\n",
    "        alignment_scores = self.attention(lstm_out,encoder_outputs)\n",
    "        # Softmaxing alignment scores to obtain Attention weights\n",
    "        attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n",
    "\n",
    "        # Multiplying Attention weights with encoder outputs to get context vector\n",
    "        context_vector = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs)\n",
    "\n",
    "        # Concatenating output from LSTM with context vector\n",
    "        output = torch.cat((lstm_out, context_vector),-1)\n",
    "        # Pass concatenated vector through Linear layer acting as a Classifier\n",
    "        output = F.log_softmax(self.classifier(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, method=\"dot\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Defining the layers/weights required depending on alignment scoring method\n",
    "        if method == \"general\":\n",
    "            self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        elif method == \"concat\":\n",
    "            self.fc = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "            self.weight = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "  \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        if self.method == \"dot\":\n",
    "            # For the dot scoring method, no weights or linear layers are involved\n",
    "            return encoder_outputs.bmm(decoder_hidden.view(1,-1,1)).squeeze(-1)\n",
    "        elif self.method == \"general\":\n",
    "            # For general scoring, decoder hidden state is passed through linear layers to introduce a weight matrix\n",
    "            out = self.fc(decoder_hidden)\n",
    "            return encoder_outputs.bmm(out.view(1,-1,1)).squeeze(-1)\n",
    "        elif self.method == \"concat\":\n",
    "            # For concat scoring, decoder hidden state and encoder outputs are concatenated first\n",
    "            out = torch.tanh(self.fc(decoder_hidden+encoder_outputs))\n",
    "            return out.bmm(self.weight.unsqueeze(-1)).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import pandas\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.fr import French\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3a3873902b04>:21: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(training_examples)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39c94cc2d5246cbbd5a19363af8a5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"fra.txt\", \"r+\") as file:\n",
    "    fra = [x[:-1] for x in file.readlines()]\n",
    "en = []\n",
    "fr = []\n",
    "for line in fra:\n",
    "    en.append(line.split(\"\\t\")[0])\n",
    "    fr.append(line.split(\"\\t\")[1])\n",
    "\n",
    "# Setting the number of training sentences we'll use\n",
    "training_examples = 10000\n",
    "# We'll be using the spaCy's English and German tokenizers\n",
    "spacy_en = English()\n",
    "spacy_fr = French()\n",
    "\n",
    "en_words = Counter()\n",
    "fr_words = Counter()\n",
    "en_inputs = []\n",
    "fr_inputs = []\n",
    "\n",
    "# Tokenizing the English and German sentences and creating our word banks for both languages\n",
    "for i in tqdm_notebook(range(training_examples)):\n",
    "    en_tokens = spacy_en(en[i])\n",
    "    fr_tokens = spacy_fr(fr[i])\n",
    "    if len(en_tokens)==0 or len(fr_tokens)==0:\n",
    "        continue\n",
    "    for token in en_tokens:\n",
    "        en_words.update([token.text.lower()])\n",
    "    en_inputs.append([token.text.lower() for token in en_tokens] + ['_EOS'])\n",
    "    for token in fr_tokens:\n",
    "        fr_words.update([token.text.lower()])\n",
    "    fr_inputs.append([token.text.lower() for token in fr_tokens] + ['_EOS'])\n",
    "\n",
    "# Assigning an index to each word token, including the Start Of String(SOS), End Of String(EOS) and Unknown(UNK) tokens\n",
    "en_words = ['_SOS','_EOS','_UNK'] + sorted(en_words,key=en_words.get,reverse=True)\n",
    "en_w2i = {o:i for i,o in enumerate(en_words)}\n",
    "en_i2w = {i:o for i,o in enumerate(en_words)}\n",
    "fr_words = ['_SOS','_EOS','_UNK'] + sorted(fr_words,key=fr_words.get,reverse=True)\n",
    "fr_w2i = {o:i for i,o in enumerate(fr_words)}\n",
    "fr_i2w = {i:o for i,o in enumerate(fr_words)}\n",
    "\n",
    "# Converting our English and German sentences to their token indexes\n",
    "for i in range(len(en_inputs)):\n",
    "    en_sentence = en_inputs[i]\n",
    "    fr_sentence = fr_inputs[i]\n",
    "    en_inputs[i] = [en_w2i[word] for word in en_sentence]\n",
    "    fr_inputs[i] = [fr_w2i[word] for word in fr_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderLSTM(len(en_words), hidden_size).to(device)\n",
    "attn = Attention(hidden_size,\"concat\")\n",
    "decoder = LuongDecoder(hidden_size,len(fr_words),attn).to(device)\n",
    "\n",
    "lr = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-7f71a29e5c40>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tk0 = tqdm_notebook(range(1,EPOCHS+1),total=EPOCHS)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659922eb986a4ba890ad4d853d7015d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-7f71a29e5c40>:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  tk1 = tqdm_notebook(enumerate(en_inputs),total=len(en_inputs),leave=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "teacher_forcing_prob = 0.5\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "tk0 = tqdm_notebook(range(1,EPOCHS+1),total=EPOCHS)\n",
    "for epoch in tk0:\n",
    "    avg_loss = 0.\n",
    "    tk1 = tqdm_notebook(enumerate(en_inputs),total=len(en_inputs),leave=False)\n",
    "    for i, sentence in tk1:\n",
    "        loss = 0.\n",
    "        h = encoder.init_hidden()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        inp = torch.tensor(sentence).unsqueeze(0).to(device)\n",
    "        encoder_outputs, h = encoder(inp,h)\n",
    "        \n",
    "        #First decoder input will be the SOS token\n",
    "        decoder_input = torch.tensor([en_w2i['_SOS']],device=device)\n",
    "        #First decoder hidden state will be last encoder hidden state\n",
    "        decoder_hidden = h\n",
    "        output = []\n",
    "        teacher_forcing = True if random.random() < teacher_forcing_prob else False\n",
    "        \n",
    "        for ii in range(len(fr_inputs[i])):\n",
    "            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Get the index value of the word with the highest score from the decoder output\n",
    "            top_value, top_index = decoder_output.topk(1)\n",
    "            if teacher_forcing:\n",
    "                decoder_input = torch.tensor([fr_inputs[i][ii]],device=device)\n",
    "            else:\n",
    "                decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "            output.append(top_index.item())\n",
    "            # Calculate the loss of the prediction against the actual word\n",
    "            loss += F.nll_loss(decoder_output.view(1,-1), torch.tensor([fr_inputs[i][ii]],device=device))\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        avg_loss += loss.item()/len(en_inputs)\n",
    "    tk0.set_postfix(loss=avg_loss)\n",
    "  # Save model after every epoch (Optional)\n",
    "torch.save({\"encoder\":encoder.state_dict(),\"decoder\":decoder.state_dict(),\"e_optimizer\":encoder_optimizer.state_dict(),\"d_optimizer\":decoder_optimizer},\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: i was busy . _EOS\n",
      "Predicted: j' ã © tais occupã © e .\n",
      "Actual: j' ã © tais occupã © e . _EOS\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "# Choose a random sentences\n",
    "i = random.randint(0,len(en_inputs)-1)\n",
    "h = encoder.init_hidden()\n",
    "inp = torch.tensor(en_inputs[i]).unsqueeze(0).to(device)\n",
    "encoder_outputs, h = encoder(inp,h)\n",
    "\n",
    "decoder_input = torch.tensor([en_w2i['_SOS']],device=device)\n",
    "decoder_hidden = h\n",
    "output = []\n",
    "attentions = []\n",
    "while True:\n",
    "    decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "    _, top_index = decoder_output.topk(1)\n",
    "    decoder_input = torch.tensor([top_index.item()],device=device)\n",
    "    # If the decoder output is the End Of Sentence token, stop decoding process\n",
    "    if top_index.item() == fr_w2i[\"_EOS\"]:\n",
    "        break\n",
    "    output.append(top_index.item())\n",
    "    attentions.append(attn_weights.squeeze().cpu().detach().numpy())\n",
    "print(\"English: \"+ \" \".join([en_i2w[x] for x in en_inputs[i]]))\n",
    "print(\"Predicted: \" + \" \".join([fr_i2w[x] for x in output]))\n",
    "print(\"Actual: \" + \" \".join([fr_i2w[x] for x in fr_inputs[i]]))\n",
    "\n",
    "# Plotting the heatmap for the Attention weights\n",
    "fig = plt.figure(figsize=(12,9))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(np.array(attentions))\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels(['']+[en_i2w[x] for x in en_inputs[i]])\n",
    "ax.set_yticklabels(['']+[fr_i2w[x] for x in output])\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
